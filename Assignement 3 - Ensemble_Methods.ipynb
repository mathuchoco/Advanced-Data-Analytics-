{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensemble Methods.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mathuchoco/Advanced-Data-Analytics-/blob/master/Assignement%203%20-%20Ensemble_Methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoaW8GB6IVzs",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cq-imyuZgVVc",
        "colab_type": "text"
      },
      "source": [
        "#Question 2 - Ensemble Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAMDOlUYgd_h",
        "colab_type": "text"
      },
      "source": [
        "## The question: \n",
        "Ensemble methods have been very successful in building classifiers. The hot topics\n",
        "include how to create diverse classifiers and how to fuse the decisions from\n",
        "individual classifiers, in particular how to establish the weights that individual\n",
        "classifiers contribute to the ensemble’s answer. Describe two existing approaches\n",
        "to solving this problem, discuss their advantages and disadvantages. Make a plan\n",
        "to address one issue or two (related to learning the weights or creating diverse\n",
        "classifiers), briefly describe your new method. Explain the reason why the\n",
        "developed method could outperform the conventional ones. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEhosEF7griz",
        "colab_type": "text"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxynHgPegvOD",
        "colab_type": "text"
      },
      "source": [
        "The derivation of classifiers has brought immense success to all industries in regards to predicting future events. Classifiers based on past existing data collates trends to make these predictions and helps businesses and industries make better decisions. There are different types of classifiers that implement this, each with it’s own functions and methods to predict outcomes to its highest accuracy. Despite this, constant research is being implemented to find ways to increase the predictive measures such as accuracy, precision, Recall, F-score. Ensemble methods enables this through its unique method of creating diverse classifiers and aggregating the decisions from them. This report will discuss the concepts of Ensemble methods, the current approaches, the advantages and disadvantages of them. A common issue conquered with the utilisation of Ensemble methods is time consumption due to the complex and long structures of the ensembles. The report also proposes a solution to overcome this problem by introducing a new solution Enhanced Boost Aggregation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lbL-fKAgyPE",
        "colab_type": "text"
      },
      "source": [
        "## About Ensemble methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOXVvwz6g3v9",
        "colab_type": "text"
      },
      "source": [
        "Ensemble methods is a supervised machine learning paradigm that bases of a hypothesis that, combining weak learners or models in the right manner can produce more accurate and robust predictive models, decreasing variance, bias and improve predictions. \n",
        "\n",
        "A predictive model produces the most optimum results when it has low bias and variance. This is controlled through bias-variance tradeoff where the amount of variance and bias has to be balanced to posses a robust model. Ensemble methods tackle this by reaching the optimal balance through constructing a set of classifiers from weak learners and classifying new data points by taking a weighted vote of the predictions from these individual classifiers.\n",
        "\n",
        "Despite the different types of ensemble approaches, all of them share the steps: \n",
        "1.\tGenerating diverse classifiers that are weak learners on the subsets of original data.  \n",
        "2.\tCombing these individual classifiers into a single hypothesis as an optimal predictive model \n",
        "\n",
        "This report will discuss two popular methods that tackles this ensemble concepts of creating diverse classifiers and fusing decisions from individual classifiers, in regards to weights called Bootstrap Aggregating (Bagging) and Boosting. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byJ2QykuhbqL",
        "colab_type": "text"
      },
      "source": [
        "## Bootstrapping Aggreagating (Bagging)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d9itPHchg5y",
        "colab_type": "text"
      },
      "source": [
        "Bagging is a powerful ensemble method that is used for unstable, high variance machine learning algorithms. It creates multiple models that are weak learners, where the different models are generated in parallel using the same algorithm with the application of bootstrapping sampling. Bootstrapping is a method that randomly samples data as a small subset from the original dataset with the ability of replacement of the subset meaning some original examples may or may not appear more than once. The bagging method then aggregates the generated model as a weighted combination through getting the average for regression or majority voting for classification. It would be logical to combine strong learners to produce a stronger model but this is less advantageous as it doesn’t assist improving the generalization performance. \n",
        "\n",
        "Random Forest is a popular method for classification and regression that follows this idea of ensemble learning. A single decision tree is a weak learner that has low bias and high variance. Random Forest combines multiple trees that are generated using bootstrapping and the results are aggregated. The low bias remains and the variance goes down achieving the optimal balance in the bias-variance trade-off.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY41qKIi_gtQ",
        "colab_type": "text"
      },
      "source": [
        "### Advantages and Disadvantages:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Otcgrmm5_lTR",
        "colab_type": "text"
      },
      "source": [
        "One of Bagging’s attractive features is the ability to reduce variance. In the context of the random forest example, even though the number of trees are increased to decrease variance and overcoming overfitting, Bagging manages to keep the bias same. The concept of averaging helps reveal real structures that exists across the whole dataset as the predictions are aggregated from random sampling with replacements. \n",
        "\n",
        "\n",
        "Bagging can easily be described as a simple algorithm that handles higher dimensionality data and learns non-linearity really well. It also handles missing values without affecting the accuracy and is an algorithm that can be trained fast. These features are what makes Bagging a popular choice as an ensemble method for predictions. As well as its attractive features, Bagging has also got some evident disadvantages. \n",
        "\n",
        "\n",
        "Bagging works well with classifiers that has got a decent prediction error in the beginning. If it is applied to bad classifiers, it degrades their performance even further. When bagging gets implemented on stable methods such as K-nearest neighbours, it doesn’t have a detrimental effect but still mildly degrades the performance. It is specific on what it can be used for and works best on unstable and high variance machine learning algorithms.  As well as the concept of average helps reveal the real structure, it doesn’t give precise values when a regression model is implemented which is a defect when looking to get precise values. \n",
        "\n",
        "\n",
        "Loss of Interpretability is a prominent disadvantage that exists in Bagging. For example, decision trees, where at every instance of decision tree fitting to the bootstrap sample, we produce a different tree and the final bagged model is not a tree which fails to be categorised as a clear interpretative ability. Bagging also enumerates high memory consumption during processes such as tree construction. This is a result of the computational complexity created when generate multiple learners, even more when the more involved implementation of prunes and validating on the original training data is included. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTfTSflrhlnB",
        "colab_type": "text"
      },
      "source": [
        "## Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHOik03Lhn7h",
        "colab_type": "text"
      },
      "source": [
        "Boosting is also a powerful tool of ensemble learning, in some cases even surpassing Bagging. It is an iterative technique that incrementally builds the ensemble by training each model with the original dataset where the weight of instances are adjusted according to the false-negative errors of the last classification. It uses these weighted averages to ‘boost’ the weak learners into strong learners hence the term ‘Boosting’. Bagging utilises a parallel learning method with each model being run independently and aggregating it in the end, but Boosting is a sequential learning of predictors where the steps are dependent on each other. Different forms of boosting algorithms focus on false-negative algorithms on resolving errors in classification models with categorical targets and numerical errors in regressions models with continuous targets.\n",
        "\n",
        "\n",
        "There are many different types of boosting techniques ranging from Gradient Boosting to XGBoost, Stochastic gradient boosting, LightGBM or CatBoost. Adaptive Boosting (Ada Boosting/ adaboost) is one of the first boosting algorithms developed for binary classification and is a popular boosting method that updates the weights attached to each of the training dataset instances and works with decision stamps. Decision stump is a unit depth tree which decides the most significant cut on features. Multiple decision stumps are made according to the mis-classified instances and these stumps are combined at the end to make a final predictor. Ada boosting can be utilised to boost any machine learning algorithm but it works best on weak learners such as decision trees.\n",
        "\n",
        "\n",
        "A overview of the Ada boosting methodology:\n",
        "- First we classify the original dataset and give equal weights to all of the instances\n",
        "\n",
        "- If the first learner incorrectly predicts the instances, then higher weight is assigned to the instances that were predicted incorrectly. This step is iterative until it reaches a limit on the number of models or accuracy. \n",
        "\n",
        "\n",
        "Each new learner’s main focus is to target the instances that are having difficulties being classified through weights. While Bagging is suitable to reduce high variance, Boosting is very apt to reduce bias error to build stronger predictive models. Boosting’s bias reduction comes from the way it adjusts its distribution over the training dataset. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4OrJ4Nu_ygW",
        "colab_type": "text"
      },
      "source": [
        "### Advantages and Disadvantages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhAwf1io_2hq",
        "colab_type": "text"
      },
      "source": [
        "In terms of Ada boosting,  as it targets the misclassified instances directly through adding weights, the final predictor produces very accurate results and often performs better than other algorithms. It can achieve similar classification results to other powerful classifiers such as SVM with more less changes to the settings. So only the most appropriate weak classifiers for the classification problem needs to be chosen, along with deciding the number of boosting rounds to be done. Ada Boosting has the flexibility to be combined with any machine learning algorithm and doesn’t have the need to tune all of the parameters. It also has the ability to be extended more than just binary classification and can be applied with text and numeric data making it versatile.\n",
        "\n",
        "\n",
        "Other boosting techniques like Gradient boosting allows direct optimisation on different types of loss functions and offers various hyperparameter tuning options allowing flexibility which is very beneficial. It also has the capability of handling missing data well where imputation and data pre-processing isn’t required.  \n",
        "\n",
        "\n",
        "Ada boosting in particular doesn’t have a lot of disadvantages. It is sensitive to extreme values, noises, especially uniform voices, and outliers. Similar to bagging, when a weak classifier that is too weak is implemented, it can lower the margins and lead to overfitting. Overfitting is a common problem in boosting that can occur when the complexity of the classification increases. The increase in complexity also makes it hard to implement in real time platforms. \n",
        "\n",
        "\n",
        "Due to its iterative process, Boosting does learn slowly and the high flexibility results has a heavy influence on the behaviour of the approach such as the number of iterations, tree depth and so on. This raises a need for large grid search during the tuning stage which is expensive. Boosting, similar to bagging is a computationally expensive process especially gradient boosting, which often requires a large amount of trees that is expensive and exhaustive in regards to time and memory. \n",
        "\n",
        "\n",
        "There are many other alternative various methods that solve these ensemble concepts. Some examples are simple averaging, stacking, bootstrap aggregating, boosting, Bayes optimal classifier, Bayesian parameter averaging. Along with Bagging and Boosting, Stacking is another popular method that works with heterogenous classifiers unlike Bagging and Boosting that deals mostly with homogenous classifiers. The core concept of stacking is to utilize a new classifier to rectify the significant errors of the previous classifier leading to a ‘Stack’ of classifiers. A learning algorithm is trained to combine the predictions of other various learning algorithms. This method equally produces accurate results and is a great alternative to either Bagging or Boosting. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r53akfdIhs4k",
        "colab_type": "text"
      },
      "source": [
        "## Current Issue and proposed solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTWa4ANZhynZ",
        "colab_type": "text"
      },
      "source": [
        "One of Boosting’s disadvantages as addressed above is the time consumption as it fixes the misclassified instances iteration by iteration. Ada Boosting adds weights to all of the misclassified instances identified so that at the next iteration those instances will be classified properly. To tackle the problem of the time consumption through reducing the amounts of weights added at each iteration, a new method called Enhanced Boost Aggregation is proposed. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-f1yEoph1Nz",
        "colab_type": "text"
      },
      "source": [
        "### Enchanced Boost Aggregation (EBA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTt077fZh6HD",
        "colab_type": "text"
      },
      "source": [
        "Enhanced boost aggregation as the name suggests is a mixture of bagging and Ada boosting. The main purpose of this proposed method is to decrease the amount of weights allocated to reduce cost and time spent on each iteration. Similar to bagging, it first conducts bootstrap sampling to generate subsets from the training dataset. Each subset is trained by different classifier. For example, lets say we have three subsets where Subset 1 is trained by K-Nearest Neighbour, Subset 2 by Random Forest and Subset 3 by Naïve Bayes. Then following Ada boosting, we take our training data to test each model to obtain the misclassified instances in each model. Instead of taking all of the misclassified observations from each of the model as Ada Boosting, we only get the unique and common instances. Following the example below, the plus symbol is supposed to be classified as black and the minus symbol is supposed to be classified as red in colour. Suppose the models produced these results below:\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1e5iGXUGXZB-YdxdE1dE2VaWle9_ixsBH)\n",
        "\n",
        "The misclassifed instances have been circled below in red:\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=10p39jC0X2FbJcHxa4fISt6IKp8XdY4uu)\n",
        " \n",
        "As the normal procedure of Ada Boosting where all of the misclassified errors are weighted, EBA only selects the instances that are incorrect in all of the models because the classifiers will be combined in the end, so there is no need to add weights to the instances that have been correctly classified by other classifiers. So that would only be: \n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1OZ--fpkjgJAzsFWzlBtHugCHYoPN15YH)\n",
        " \n",
        "For the next iteration these instances will be weighted higher to be correctly classified at the next iteration. Then different sample subsets for the next iteration is generated again for each models KNN, Random Forest and Naïve Bayes and the same process is repeated until the common unique errors are reduced completely. Finally similar to Ada Boosting, all three learners at the end will be fused together into a final classifier by majority vote. \n",
        "\n",
        "\n",
        "The use of three different classifiers can be expensive as it adds complexity but the implementation of this will significantly reduce the amount of iteration and weights added in the current Ada Boosting technique. Boosting is known to reduce bias significantly and bagging is known to reduce variance. The combination of both techniques along with overcoming the expense caused by time and higher iteration will cause the reduction in bias and variance leading to an optimal balance in the bias-variance trade-off. If implemented successfully, EBA has high potential to outperform the conventional methods as it combines the best of Bagging and Boosting, two successful methods.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSx8O3qAh-5O",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbHBYt2niAoT",
        "colab_type": "text"
      },
      "source": [
        "Ensemble methods are a successful set of algorithms that has produced impressive results and are used vastly in competitions like Kaggle. It deploys the basic concept of combing weak learners to produce a stronger learner to improve the prediction measure statistics. Ensemble methods such as Bagging and Boosting have seen success in various fields such as Computer Vision, Facial recognition, Speech processing, Biology to name a few. Despite its disadvantages of being time and memory expensive, complex models, loss of interpretability, its advantages clearly overtake this as it enables us to obtain high predictive models due to its ability to obtain low bias-variance trade-off.  A new method, Enhanced Boost Aggregation has been proposed to tackle the problem of time consumption through reducing the allocation of weights during each iteration of boosting. This method with the correct implementation has the ability to surpass the current methods as it combines the best of the best methods. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hd5ZxVbjDHDf",
        "colab_type": "text"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcG1JMLJDJEO",
        "colab_type": "text"
      },
      "source": [
        "- Ravanshad, A. (2019). Ensemble Methods. [online] Medium. Available at: https://medium.com/@aravanshad/ensemble-methods-95533944783f [Accessed 7 Oct. 2019].\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- Opitz, D. and Maclin, R. (1999). Popular Ensemble Methods: An Empirical Study. Journal of Artificial Intelligence Research, 11, pp.169-198.\n",
        "DEMIR, N. (2019). Ensemble Methods: Elegant Techniques to Produce Improved Machine Learning Results. [online] Toptal Engineering Blog. Available at: https://www.toptal.com/machine-learning/ensemble-methods-machine-learning [Accessed 8 Oct. 2019].\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- Dietterich, T. Ensemble Methods in Machine Learning. Oregon State University, Corvallis, Oregon, USA, 11, pp. 1-13\n",
        "\n",
        "\n",
        "- Rocca, J. (2019). Ensemble methods: bagging, boosting and stacking. [online] Medium. Available at: https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205 [Accessed 7 Oct. 2019].\n",
        "\n",
        "\n",
        "- Machova, K., Puszta, M., Barcak, F. and Bednar, P. (2006). A comparison of the bagging and the boosting methods using the decision trees classifiers. Computer Science and Information Systems, 3(2), pp.57-72.\n"
      ]
    }
  ]
}