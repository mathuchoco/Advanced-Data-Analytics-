{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML- Assignment 1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mathuchoco/Advanced-Data-Analytics-/blob/master/ML_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3WLQaTKbDCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u22Bd4NOcgx2",
        "colab_type": "text"
      },
      "source": [
        "#Assignment 1 - Understanding the literature: Long short-term memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNXRTYZakhLA",
        "colab_type": "text"
      },
      "source": [
        "Github Link to the report: https://github.com/mathuchoco/Advanced-Data-Analytics-/blob/master/ML_Assignment_1.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfoS0dcRbU3U",
        "colab_type": "text"
      },
      "source": [
        "###Introduction\n",
        "\n",
        "This critique evaluates the solution proposed by Sepp Hochreiter and Jurgen Schmidhber, published in the year of 1997 named Long Short Term Memory (LSTM). This research paper aims to tackle the problem of vanishing and exploding gradients experienced when training Recurrent Neural Networks (RNN) through back propagation, and solve complex long time lag tasks more efficiently. This paper is critiqued based on its Content, Innovation, Technical Quality, how its applied and if any X-factor can be observed, and finally the presentation. \n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe1_UiEDb_Aj",
        "colab_type": "text"
      },
      "source": [
        "###Content \n",
        "\n",
        "Long Short Term memory is introduced as an architecture with the purpose of solving the vanishing and exploding gradients (values that update the neural network weight) experienced by Recurrent Neural Networks, and solve complex, artificial long time lag tasks to allow sequence prediction to be possible and easier. \n",
        "\n",
        "When training RNN using back-propagation (going back time stamps to modify weights), the back-propagated error decays exponentially and depends on the size of the weights.  When there are oscillating sizes of weights, it explodes. When they(RNN) are learning to bridge long time lags, they have partial derivation of error in regards to weight being less than 1 and this takes an unreasonable amount of time where they either don’t work, shrink or it vanishes as it back-propagates in time. This is the main challenge posed that is a setback for efficient sequence prediction. LSTM isn’t a brand-new solution, but rather a modulated RNN. The paper is well structured as it takes you the problem and the process of how LSTM was derived sequentially. \n",
        "\n",
        "To overcome this challenge, neural networks need to allow constant error flow.  The researchers implement this by extending the Constant error carrousel (CEC) through ‘special, self-connected units without the disadvantages of the naïve approach’ (Hochreiter and Schmidhber, 1997). They introduce a multiplicative input and output gate unit where both have a similar functionality of protecting units from getting in contact with unnecessary inputs. The input gate protects the memory contents stored within j (a single unit) while the output gate protects the other units. The memory cell is constructed around CEC which is the unchanged cell state that carries information throughout. The gates contain sigmoid activation which transforms the information between 0 and 1 based on 0 being not important and 1 as important. The input and cell state usually gets transformed by another activation function called tanh. \n",
        "\n",
        "This paper showcases the basic LSTM architecture, as since its publication there has been many modifications and inclusions made to the structure such as the Forget Gate, Peepholes.  The architecture explored in this paper contains three main layers, Input, Hidden state and output. The researchers test their architecture through multiple long time lag experiment tasks with increasing difficulties. Even though LSTM isn’t the fastest solution for some experiments like Experiment 3 where guessing random weights proved to be faster, LSTM succeeds making sequence predictions. It is highly commendable that LSTM not only solve the problem it aimed to solve with the gradients but also solve complex long time lag tasks in a more efficient manner making sequence prediction easier and effective. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFOoO35jcFph",
        "colab_type": "text"
      },
      "source": [
        "###Innovation\n",
        "\n",
        "As a subjective term, innovation can be described through its synonyms: Novelty, Invention, Revolutionary, Improvement. LSTM fulfils all these synonyms with ease if they were to be used as a criterion for innovation. Before the age of LSTM, Recurrent Neural Network itself was seen as an innovative solution as it was the first of State of Art algorithms to memorize and keep previous inputs in memory, even with huge datasets. This was the beginning of the ability to make sequence predictions, one of the hardest data science predictions possible. So, it was a huge setback when it encountered vanishing and exploding gradients during backpropagation. Introducing the LSTM architecture, during this timeframe became revolutionary as LSTM’s not only fixed issues of backpropagation but it’s ability to learn when to forget and remember things, and allow constant error flow have made it one of the biggest turning points as a solution for sequence prediction, in the history of neural networks. It solves ‘complex, artificial long time lag tasks that have never been solved by previous recurrent network algorithms’ (Hochreiter and Schmidhber, 1997) Since the introduction of LSTM, there has been many success stories till this date and many different successful applications and concepts utilise LSTM (See Application and X-Factor). \n",
        "\n",
        "Even though it is based on RNN, LSTM can easily be classified as one of the most profound novelties of the late 90s in the neural network sector. This statement is on the pure basis of CECs, Memory Cells and Gate Units. These three aspects of the architecture allow constant error flow and effectual way for the information to get passed through which is what defines LSTM from traditional RNN. The researchers showcase the uniqueness of LSTM through the constant comparison to all previous work and past solutions of that time throughout the paper. This comparison labels how LSTM is the most unique and ‘best’ solution of that time for sequence prediction. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOJ3aqNocLv4",
        "colab_type": "text"
      },
      "source": [
        "###Technical Quality \n",
        "\n",
        "Long Short term memory is a technical research paper with exceptional technical quality. It is not just a research paper that introduces a new concept but rather a paper composed to present a technical solution for a very technical problem. \n",
        "\n",
        "Firstly, this paper presents the technical concept of LSTM remarkably throughout the paper in a sequential order with constant references to its appropriate sources. The researchers introduce the problem, followed by a brief through all the previous works in the field with the comparison to LSTMs. They make a detailed analysis of the constant error back prop (the problem) with mathematical explanations right to the point where the formulas showcase the explosion and vanishing of the gradients. These algorithms are also further clarified in the appendix sections. Long short term memory is then introduced in detail as a next step to the problem with appropriate algorithms and calculations for verification and diagrams of the architecture for clarification. This order of information made it easier to understand without needing to go back and forth for clarification. \n",
        "\n",
        "The technicality of this research is displayed through the experiment section of six increasingly difficult experiments. Even though this section was as excellent as the others, it was highly technical to a point where it was confusing to understand in some places for me personally. During each experiment, the architecture/training and testing explanations, the researchers could’ve included a diagram of the architecture, as done in the previous section to help visual learners like me understand better. \n",
        "\n",
        "For the first two experiments the researchers include RTRL results as a benchmark to showcase how LSTM exceeds those results. But for the first experiment, rather than performing the task with the RTRL as they did in experiment two, the RTRL results used were taken from ‘Smith and Zipser 1989.’ This was the only experiment where they took the results from another paper. It makes sense, as Embedded Reber grammar is a popular and a standard test for recurrent networks, but to maintain their high technical quality, the RTRL results could’ve been generated by Hochreiter and Schmidhber. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7tTnWrGcPlT",
        "colab_type": "text"
      },
      "source": [
        "###Application and X-factor \n",
        "\n",
        "This paper’s proposed architecture intends solve problems to enable an easier path for Sequence predictions. But sequence prediction is vast concept that is too generic to be defined and confined as a single application domain as every predictive problem contains a sequence. Text generation and comprehension is more suitable to be classified as the application domain as this was the outcome the researchers were trying to achieve in all of their different scenario experiments. \n",
        "\n",
        "LSTM as it is highly efficient, not only solves the gradient problem and makes text generation proficient, it also has made many other functionalities and application domains possible. There are many other application domains where LSTM can be successfully applied: Language Modelling, Machine Translation, Image Captioning, Hand writing generation, Music compositions, Speech Recognition, Protein homology detection. \n",
        "Despite the high performance and advantages of Long-short term memory, like everything, LSTM also has its limitations and setbacks. LSTM’s chunkiness with its gates and cells will make the training time will longer. The model also seems to require a large amount of memory to train due to its multiple layers. Extending the research area to find ways to overcome these limitations will be extremely beneficial. Another area of exploration can be into predicting brain behaviour and cancer behaviour for better diagnosis. Behaviour and sequences are interlinked and as brain and cancer are two areas that are still not completely solved and involve large amount of sequences, these research areas could use an effective sequence predictive model as LSTM. Other areas can be space where LSTM can be developed to translate information that satellites, rovers or any space crafts that are sent out in the universe provide more, efficiently. This architecture has high potential and is versatile that the opportunities are endless!\n",
        " \n",
        "Something I found particularly interesting other than its versatility and how it was a solution that created many, was how LSTM succeeded all of the experiments conducted by the researchers. It was slightly slower in a few experiments but none of them failed or hit below expectations and it solved the different types of experiments that has never been solved successfully with the same architecture. \n",
        "\n",
        "As of current, there are some articles that are advising against LSTM and comparing to ResNet and Attention which seem to be producing more effective results. But then LSTM has also helped produce amazing results for speech to text comprehension and helped raise products like Siri, Cortana, Alexa. So, in the context of 2019, this will be quite a topic to talk about in class!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybty68flcSq2",
        "colab_type": "text"
      },
      "source": [
        "###Presentation \n",
        "\n",
        "Presentation can easily be added to this paper’s list of exceptional aspects. Apart from the interesting concepts, I loved reading the article for its presentation. The paper was well structured in a way that it took the readers sequentially into the world of sequence prediction! There was clear statement of the problem, followed by the remedy and outline of paper in the introduction which was perfect. The inclusion of previous work and constant error back prop (the problem) set it up well to properly understand why we need LSTM as a solution. This was finally followed by the experiments which helped see how effective LSTM are in differing scenarios and difficulties. The appendix at the end was very useful for understanding the highly technical algorithms as it explained all the mathematical explanations in detail. No part of the paper was overdone, but as mentioned before, in Section 5: Experiments, the architecture part could’ve added a diagram for helping understand the concept better. The algorithms throughout the article was clearly explained and wasn’t cluttered any where. The structure, layout and the presentation made the whole paper more enticing to read. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TGzxVTxcX0f",
        "colab_type": "text"
      },
      "source": [
        "###Conclusion\n",
        "Overall, it was enjoyable to read a paper that caused and was the reason for so many different inventions and concepts that we have today. The whole concept of LSTM’s ability to successfully make sequence predictions in different scenarios was very interesting. The paper was highly technical which I found it difficult to understand in some places but background research helped and made it easier to understand. All in all, LSTM seems to be an excellent architecture for sequence predictions and I highly recommend this model. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8C71rtUcZpp",
        "colab_type": "text"
      },
      "source": [
        "###References\n",
        "\n",
        "Amidi, S. (2019). CS 230 - Recurrent Neural Networks Cheatsheet. [online] Stanford.edu. Available at: https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks [Accessed 25 Aug. 2019].\n",
        "\n",
        "Staudemeyer, Ralf & Omlin, Christian. (2013). Evaluating performance of long short-term memory recurrent neural networks on intrusion detection data. ACM International Conference Proceeding Series. 218. 10.1145/2513456.2513490.\n",
        "\n",
        "Soutner, Daniel & Müller, Luděk. (2013). Application of LSTM Neural Networks in Language Modelling. 8082. 105-112. 10.1007/978-3-642-40585-3_14.\n",
        "\n",
        "Nicholson, C. (2019). A Beginner's Guide to LSTMs and Recurrent Neural Networks. [online] Skymind. Available at: https://skymind.ai/wiki/lstm [Accessed 28 Aug. 2019].\n",
        "\n",
        "Culurciello, E. (2019). The fall of RNN / LSTM. [online] Medium. Available at: https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0 [Accessed 27 Aug. 2019]."
      ]
    }
  ]
}